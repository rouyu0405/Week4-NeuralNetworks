{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rouyu0405/Week4-NeuralNetworks/blob/newBranch/week4__Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKtwydx6Z7BR",
        "outputId": "1f901842-84c3-40ce-c174-87b3e35e5a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5CvOjjrL9gw"
      },
      "source": [
        "<center><h1> Introduction to Audio Classification with Neural Networks</h1></center>\n",
        "\n",
        "# Abstract\n",
        "\n",
        "### Purpose\n",
        "This notebook serves as an introduction to working with audio data for classification problems; it is meant as a learning resource rather than a demonstration of the state-of-the-art. The techniques mentioned in this notebook apply not only to classification problems, but to regression problems and problems dealing with other types of input data as well. I focus particularly on feature engineering techniques for audio data and provide an in-depth look at the logic, concepts, and properties of the Multilayer Perceptron (MLP) model, an ancestor and the origin of deep neural networks (DNNs) today. I also provide an introduction to a few key machine learning models and the logic in choosing their hyperparameters. These objectives are framed by the task of recognizing emotion from snippets of speech audio.\n",
        "\n",
        "### Summary\n",
        "Data cleansing and feature engineering comprise the most crucial aspect of preparing machine and deep learning models alike and is often the difference between success and failure. We can drastically improve the performance of a model with proper attention paid to feature engineering. This stands for input data which is already useable for predictions; even such data can be transformed in myriad ways to improve predictive performance. For features to be useful in classification they must encompass sufficient variance between different classes. We can further improve the performance of our models by understanding the influence of and precisely tuning their hyperparameters, for which there are algorithmic aids such as Grid Search.\n",
        "\n",
        "Network architecture is a critical factor in determining the computational complexity of DNNs; often, however, simpler models with just one hidden layer perform better than more complicated models. The importance of proper model evaluation cannot be overstressed: training data should be used strictly for training a model, validation data strictly for tuning a model, and test data strictly to evaluate a model once it is tuned - a model should never be tuned to perform better on test data. To this end, K-Fold Cross Validation is a staple tool.\n",
        "\n",
        "### Conclusions\n",
        "Classic machine learning models such as Support Vector Machines (SVM), k Nearest Neighbours (kNN), and Random Forests have distinct advantages to deep neural networks in many tasks but do not match the performance of even the simplest deep neural network in the task of audio classification. The Multilayer Perceptron (MLP) model is the simplest form of DNN suited to classification tasks, provides decent off-the-shelf performance, and can be precisely tuned to be accurate and relatively quick to train.\n",
        "\n",
        "The MLP provides appreciable accuracy on the RAVDESS dataset, but suffers from the relatively small number of training samples afforded by this dataset. Long Short Term Memory Recurrent Neural Networks (LSTM RNNs) and Convolutional Neural Networks (CNNs) are excellent DNN candidates for audio data classification: LSTM RNNs because of their excellent ability to interpret sequential data such as the audio waveform represented as a time series, and CNNs because features engineered on audio data such as spectrograms have marked resemblance to images, in which CNNs excel at recognition and discrimination between distinct patterns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQsTfGREL9g1"
      },
      "source": [
        "<!--TABLE OF CONTENTS-->\n",
        "\n",
        "\n",
        "# Table of Contents\n",
        "  - [Intro: Speech Emotion Recognition on the RAVDESS dataset](#Intro:-Speech-Emotion-Recognition-on-the-RAVDESS-dataset)\n",
        "  - [Machine Learning Process Overview](#Machine-Learning-Process-Overview)\n",
        "  - [Feature Extraction](#Feature-Extraction)\n",
        "    - [Load the Dataset and Compute Features](#Load-the-Dataset-and-Compute-Features)\n",
        "    - [Feature Scaling](#Feature-Scaling)\n",
        "  - [The MLP Model for Classification](#The-MLP-Model-for-Classification)\n",
        "    - [Choice of Hyperparameters](#Choice-of-Hyperparameters)\n",
        "    - [Network Architecture](#Network-Architecture)\n",
        "    - [Hyperparameter Optimization and Grid Search](#Hyperparameter-Optimization-and-Grid-Search)\n",
        "  - [Training and Evaluating the MLP Model](#Training-and-Evaluating-the-MLP-Model)\n",
        "    - [The Confusion Matrix](#The-Confusion-Matrix)\n",
        "    - [Precision, Recall, F-Score](#Precision,-Recall,-F-Score)\n",
        "    - [K-Fold Cross-Validation](#K-Fold-Cross-Validation)\n",
        "    - [The Validation Curve: Further Tuning of Hyperparameters](#The-Validation-Curve:-Further-Tuning-of-Hyperparameters)\n",
        "    - [The Learning Curve: Determining Optimal Training Set Size](#The-Learning-Curve:-Determining-Optimal-Training-Set-Size)\n",
        "  - [Higher Complexity DNNs](#Higher-Complexity-DNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZUcbr4PL9g2"
      },
      "source": [
        "## Intro: Speech Emotion Recognition on the RAVDESS dataset\n",
        "In this notebook, we train a Multilayer Perceptron (MLP) model for classification in an attempt to recognize the emotion conveyed in a speech audio snippet. MLP classifiers are a good DNN model to start with because they are simple, flexible, and suited when inputs are assigned a label - in our case, emotion.\n",
        "\n",
        "We're going to use the RAVDESS dataset (Ryerson Audio-Visual Database of Emotional Speech and Song dataset), created by Steven Livingstone and Frank Russo of Ryerson University. <br>\n",
        "[Details of the RAVDESS dataset](https://smartlaboratory.org/ravdess/) <br>\n",
        "[Download the dataset used in this notebook](https://1sfu-my.sharepoint.com/:f:/g/personal/oyalcin_sfu_ca/ErKIxg5g4rFOlfrAZ352DW4BD1ytBiz1kZLcj5Elk9_1rQ?e=lgUQoi) <br> Scroll half-way down the page and find \"Audio_Speech_Actors_01-24\"<br>\n",
        "\n",
        "We're going to use the audio-only speech portion of the RAVDESS dataset, ~200MB.\n",
        "Audio is sourced from 24 actors (12 male, 12 female) repeating two sentences with\n",
        "a variety of emotions and intensity. We get 1440 speech files (24 actors * 60 recordings per actor). Each audio sample has been rated  by a human 10 times for emotional quality.\n",
        "\n",
        "## Machine Learning Process Overview\n",
        "1. Feature Engineering: Choose and define the properties which our model will use to evaluate the audio files. <br>\n",
        "2. Feature Extraction: Compute the features for each audio file and build a feature matrix representing all audio files. <br>\n",
        "3. Model exploration: Test candidate models that make sense for the properies of the dataset\n",
        "4. Training the MLP Classifier model: Choose and optimize the properties of our model on validation data - hyperparameters and architechture.  <br>\n",
        "5. Evaluate our model's performance: Evaluate our model's accuracy on validation data and score it against test data which it has never seen in training.<br>\n",
        "6. Explore options for improving our model: Is our dataset the right size? Is our model too complex or too simple? <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zg6HC4zIaZKm"
      },
      "outputs": [],
      "source": [
        "#importing the required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import soundfile\n",
        "import os\n",
        "# matplotlib complains about the behaviour of librosa.display, so we'll ignore those warnings:\n",
        "import warnings; warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu8VPhDmL9hC"
      },
      "source": [
        "## Feature Extraction\n",
        "We're going to repeat the feature extraction process from previous weeks, and calculate the following:\n",
        "\n",
        "**Chromagram**: Will produce 12 features; One for each of 12 pitch classes\n",
        "\n",
        "**Mel Spectrogram**: Will produce 128 features; We've defined the number of mel frequency bands at n_mels=128\n",
        "\n",
        "**MFCC**: Will produce 40 MFCCs; I've set the number of coefficients to return at n_mfcc=40 which I found to work well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qTe93WYTL9hD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def feature_chromagram(waveform, sample_rate):\n",
        "    # STFT computed here explicitly; mel spectrogram and MFCC functions do this under the hood\n",
        "    stft_spectrogram=np.abs(librosa.stft(waveform))\n",
        "    # Produce the chromagram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    chromagram=np.mean(librosa.feature.chroma_stft(S=stft_spectrogram, sr=sample_rate).T,axis=0)\n",
        "    return chromagram\n",
        "\n",
        "def feature_melspectrogram(waveform, sample_rate):\n",
        "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
        "    melspectrogram=np.mean(librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=128, fmax=8000).T,axis=0)\n",
        "    return melspectrogram\n",
        "\n",
        "def feature_mfcc(waveform, sample_rate):\n",
        "    # Compute the MFCCs for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # 40 filterbanks = 40 coefficients\n",
        "    mfc_coefficients=np.mean(librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "    return mfc_coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjFXIfC2L9hD"
      },
      "source": [
        "We're going to wrap our feature extraction functions so we only have to load each audio file once. After extracting our 3 audio features as NumPy arrays representing a time series, we're going to\n",
        "stack them horizontally to create a single feature array."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_waveform(waveform):\n",
        "    # If the waveform has 2 channels (stereo), convert it to mono\n",
        "    if len(waveform.shape) > 1:\n",
        "        waveform = librosa.to_mono(waveform)\n",
        "    return waveform"
      ],
      "metadata": {
        "id": "jTQHdf5Dpl0g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xPMw9ijJL9hE"
      },
      "outputs": [],
      "source": [
        "def get_features(file):\n",
        "    # load an individual soundfile\n",
        "     with soundfile.SoundFile(file) as audio:\n",
        "        waveform = audio.read(dtype=\"float32\")\n",
        "        sample_rate = audio.samplerate\n",
        "        # make sure the file is mono channel audio\n",
        "        waveform = preprocess_waveform(waveform)\n",
        "        # compute features of soundfile\n",
        "        chromagram = feature_chromagram(waveform, sample_rate)\n",
        "        melspectrogram = feature_melspectrogram(waveform, sample_rate)\n",
        "        mfc_coefficients = feature_mfcc(waveform, sample_rate)\n",
        "\n",
        "        feature_matrix=np.array([])\n",
        "        # use np.hstack to stack our feature arrays horizontally to create a feature matrix\n",
        "        feature_matrix = np.hstack((chromagram, melspectrogram, mfc_coefficients))\n",
        "\n",
        "        return feature_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-C6g6psL9hE"
      },
      "source": [
        "### Load the Dataset and Compute Features\n",
        "We have to understand the labelling of the RAVDESS dataset to find the ground truth emotion for each sample.\n",
        "Each file is labelled with 7 numbers delimited by a \"-\".\n",
        "Most of the numbers describe metadata about the audio samples such as their format (video and/or audio),\n",
        "whether the audio is a song or statement, which of two statements is being read and by which actor.\n",
        "\n",
        "The third and fourth numbers pertain to the emotional quality of each sample. The third number is in the range of 1-8 with each number representing an emotion.\n",
        "The fourth number is either 1 or 2, representing normal (1) or strong (2) emotional intensity.\n",
        "\n",
        "We're going to define a dictionary based on the third number (emotion) and assign an emotion to each number as specified by the RAVDESS dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z-Pu_fB7L9hF"
      },
      "outputs": [],
      "source": [
        "#Emotions in the RAVDESS dataset\n",
        "emotions_dict ={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTlsUOwXL9hF"
      },
      "source": [
        "Finally, let's load our entire dataset and compute the features of each audio file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mAh2AYMpL9hF"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "\n",
        "def load_data():\n",
        "    X,y=[],[]\n",
        "    count = 0\n",
        "    for file in glob.glob(\"/content/drive/MyDrive/IAT Courses/IAT 360/Assignment Data/OneDrive_2025-09-15/Audio Data/*/*.wav\"):\n",
        "        file_name=os.path.basename(file)\n",
        "        emotion=emotions_dict[file_name.split(\"-\")[2]]\n",
        "        features = get_features(file)\n",
        "        X.append(features)\n",
        "        y.append(emotion)\n",
        "        count += 1\n",
        "        # '\\r' + end='' results in printing over same line\n",
        "        print('\\r' + f' Processed {count}/{1440} audio samples',end=' ')\n",
        "    # Return arrays to plug into sklearn's cross-validation algorithms\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cf8q4K5L9hG"
      },
      "source": [
        "Compute the feature matrix and read the emotion labels for the entire dataset.\n",
        "Note that our regressor (independent/explanatory variable), usually denoted X, is named 'features', and our regressand (dependent variable), usually denoted y, is named 'emotions'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeKE591aL9hG",
        "outputId": "7c8716f2-660c-4fe0-acb7-edd2fbb426ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 1441/1440 audio samples "
          ]
        }
      ],
      "source": [
        "features, emotions = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzE6ZwqxoRVM"
      },
      "source": [
        "We're going to create dataframes of both features and emotions, and save them into .csv, not to do this again. You can also use .csv's from prior weeks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2tE2z85ioRVS",
        "outputId": "7492aafd-efd3-4490-cc2b-755f4d6fdab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Audio samples represented: 1441\n",
            "Numerical features extracted per sample: 180\n"
          ]
        }
      ],
      "source": [
        "print(f'\\nAudio samples represented: {features.shape[0]}')\n",
        "print(f'Numerical features extracted per sample: {features.shape[1]}')\n",
        "features_df = pd.DataFrame(features) # make it pretty for display\n",
        "features_df.to_csv('featuresRavdess.csv')\n",
        "\n",
        "#making dataframe for emotions as well\n",
        "emotions_df = pd.DataFrame(emotions) # make it pretty for display\n",
        "emotions_df.to_csv('emotionsRavdess.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpE5m-5aEyoB"
      },
      "source": [
        "## Load pre-saved Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHPB7dCqEotR"
      },
      "source": [
        "Once saved you only need to load them later by running the cell below, and **skip every cell above** except for the one in which we import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-8nlJCESEn56"
      },
      "outputs": [],
      "source": [
        "features=pd.read_csv('featuresRavdess.csv',index_col=0)\n",
        "emotions=pd.read_csv('emotionsRavdess.csv',index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load actor 25 csv"
      ],
      "metadata": {
        "id": "kcJAJdjlie9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVUa7RAuL9hG"
      },
      "source": [
        "Let's see what the features we extracted look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzxX583yL9hG",
        "outputId": "783e7da9-c453-4896-cb0c-47170c7a853d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Audio samples represented: 1441\n",
            "Numerical features extracted per sample: 180\n"
          ]
        }
      ],
      "source": [
        "print(f'\\nAudio samples represented: {features.shape[0]}')\n",
        "print(f'Numerical features extracted per sample: {features.shape[1]}')\n",
        "features_df = pd.DataFrame(features) # make it pretty for display\n",
        "labels_df=pd.DataFrame(emotions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq083zLEL9hH"
      },
      "source": [
        "We have a matrix of dim 1435 x 180. Looks good - 1435 audio samples, one per row, with a series of\n",
        "180 numerical features for each sample.\n",
        "\n",
        "**Each of the 1435 feature arrays has 180 features composed of 12 chromagram pitch classes + 128 mel spectrogram bands + 40 MFC coefficients.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhtlYshOL9hV"
      },
      "source": [
        "### Feature Scaling\n",
        "We're going to also do feature scaling, similar to what we did in the last weeks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IUm1DLwL9hW",
        "outputId": "43464338-e0d4-4aed-c41e-ad37db896ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 Chromagram features:           min = 0.310,     max = 1.000,     mean = 0.667,     deviation = 0.088\n",
            "\n",
            "128 Mel Spectrogram features:     min = 0.000,     max = 149.208,     mean = 0.188,     deviation = 1.603\n",
            "\n",
            "40 MFCC features:                 min = -1131.371,    max = 115.126,    mean = -15.077,    deviation = 100.125\n"
          ]
        }
      ],
      "source": [
        "# We would usually use df.describe(), but it provides a bit of a mess of information we don't need at the moment.\n",
        "def print_features(df):\n",
        "    # Check chromagram feature values\n",
        "    features_df_chromagram = df.iloc[:,:11]\n",
        "    chroma_min = features_df_chromagram.min().min()\n",
        "    chroma_max = features_df_chromagram.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    chroma_mean = features_df_chromagram.stack().mean()\n",
        "    chroma_stdev = features_df_chromagram.stack().std()\n",
        "    print(f'12 Chromagram features:       \\\n",
        "    min = {chroma_min:.3f}, \\\n",
        "    max = {chroma_max:.3f}, \\\n",
        "    mean = {chroma_mean:.3f}, \\\n",
        "    deviation = {chroma_stdev:.3f}')\n",
        "\n",
        "    # Check mel spectrogram feature values\n",
        "    features_df_melspectrogram = df.iloc[:,12:139]\n",
        "    mel_min = features_df_melspectrogram.min().min()\n",
        "    mel_max = features_df_melspectrogram.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    mel_mean = features_df_melspectrogram.stack().mean()\n",
        "    mel_stdev = features_df_melspectrogram.stack().std()\n",
        "    print(f'\\n128 Mel Spectrogram features: \\\n",
        "    min = {mel_min:.3f}, \\\n",
        "    max = {mel_max:.3f}, \\\n",
        "    mean = {mel_mean:.3f}, \\\n",
        "    deviation = {mel_stdev:.3f}')\n",
        "\n",
        "    # Check MFCC feature values\n",
        "    features_df_mfcc = df.iloc[:,140:179]\n",
        "    mfcc_min = features_df_mfcc.min().min()\n",
        "    mfcc_max = features_df_mfcc.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    mfcc_mean = features_df_mfcc.stack().mean()\n",
        "    mfcc_stdev = features_df_mfcc.stack().std()\n",
        "    print(f'\\n40 MFCC features:             \\\n",
        "    min = {mfcc_min:.3f},\\\n",
        "    max = {mfcc_max:.3f},\\\n",
        "    mean = {mfcc_mean:.3f},\\\n",
        "    deviation = {mfcc_stdev:.3f}')\n",
        "\n",
        "print_features(features_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFgQlYSlL9hW"
      },
      "source": [
        "**There's an obvious imbalance in the variance our features; Our features indeed belong to very different distributions:** our MFC coefficients' deviation is greater than the other features by orders of magnitude. That does not mean MFC coefficients are the most important feature, but rather it is a property of the way they are computed. We will certainly need to scale this feature set.\n",
        "\n",
        "We have the choice of sklearn's StandardScaler and MinMaxScaler.\n",
        "In practice, **MinMax scaling is especially useful when we know our features should be in a bounded interval**, such as pixel values in [0,255], while **standard scaling is perhaps more practical for features with unknown distributions** because centering the features at zero-mean with a standard deviation of 1 means extreme values will have less of an impact on the model's learned weights, i.e. the model is less sensitive to outliers.\n",
        "\n",
        "We'll create MinMax scaled features as well so we can give them a try later on to confirm that standard scaling is better in the absence of knowledge on the appropriate distribution for a dataset's features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_BCAYVEUL9hW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# keep our unscaled features just in case we need to process them alternatively\n",
        "features_scaled = features\n",
        "features_scaled = scaler.fit_transform(features_scaled)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# keep our unscaled features just in case we need to process them alternatively\n",
        "features_minmax = features\n",
        "features_minmax = scaler.fit_transform(features_minmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ATx5oNL9hX"
      },
      "source": [
        "Make sure our features are properly scaled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlRuHQkKL9hX",
        "outputId": "96ef7b6f-6e7f-46f7-db09-7224778ee945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mStandard Scaling:\n",
            "\u001b[0m\n",
            "12 Chromagram features:           min = -3.899,     max = 4.371,     mean = 0.000,     deviation = 1.000\n",
            "\n",
            "128 Mel Spectrogram features:     min = -0.474,     max = 36.556,     mean = -0.000,     deviation = 1.000\n",
            "\n",
            "40 MFCC features:                 min = -4.801,    max = 6.241,    mean = -0.000,    deviation = 1.000\n",
            "\n",
            "\n",
            "\u001b[1mMinMax Scaling:\n",
            "\u001b[0m\n",
            "12 Chromagram features:           min = 0.000,     max = 1.000,     mean = 0.478,     deviation = 0.145\n",
            "\n",
            "128 Mel Spectrogram features:     min = 0.000,     max = 1.000,     mean = 0.014,     deviation = 0.060\n",
            "\n",
            "40 MFCC features:                 min = 0.000,    max = 1.000,    mean = 0.413,    deviation = 0.170\n"
          ]
        }
      ],
      "source": [
        "print('\\033[1m'+'Standard Scaling:\\n'+'\\033[0m')\n",
        "features_scaled_df = pd.DataFrame(features_scaled)\n",
        "print_features(features_scaled_df)\n",
        "\n",
        "print('\\n\\n\\033[1m'+'MinMax Scaling:\\n'+'\\033[0m')\n",
        "features_minmax_df = pd.DataFrame(features_minmax)\n",
        "print_features(features_minmax_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZfzIh7DL9hX"
      },
      "source": [
        "Perfect. Zero mean and unit variance for standard scaling and in the range [0,1] for MinMax scaling - a default when we don't specify values. We can now move on to building predictive models for these features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUhPmqKbL9hi"
      },
      "source": [
        "## The MLP Model for Classification\n",
        "We're going to first try a Multilayer Perceptron (MLP) Classifier, a simple artificial neural network (ANN) model well-suited for predictions trained on labelled inputs. Note that an MLP model can be trained for regression just as well. An MLP network consists of an input layer, _n_ hidden layers, and an output layer. The logic of the network depends on its weights: An array [w<sub>0</sub>-w<sub>i</sub>] for _each node_ in all layers, one weight value for each node-node connection (edge) in the graph representation (a matrix _W_ for the network). We also have biases, an array [b<sub>0</sub>-b<sub>j</sub>] for _each layer_, one value for each node (a matrix _B_ for the network). I'll speak more to its architechture below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ShGGg4IL9hj"
      },
      "source": [
        "<img src=\"https://github.com/IAT-ExploringAI-2024/Week4-NeuralNetworks/blob/main/mlp.png?raw=true\" width=500 height=500 />\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl2GcFlIA9Lv"
      },
      "source": [
        "###Training: The 80/20 Split and Validation\n",
        "In order to compare models, we'll have to evaluate their performance. The simplest method to do so is to train a model on a portion of our dataset and test it on the remainder. We'll use sklearn's train_test_split to create a standard 80/20 train/test split. The model is fit on 80% of\n",
        "the data and tested for performance against 20% of the data, which it has never seen in training - also called the hold-out set.\n",
        "\n",
        "More accurately, the proper modality for training and scoring a model is to\n",
        "1. Fit/train our model on a _training_ set,\n",
        "2. Evaluate the model on a _validation_ set to tune the hyperparameters for better performance,\n",
        "3. Finally score our model's true performance - its **generalizability** - against a _test_ set, aka the hold-out set.\n",
        "4. Repeat from 2. **Do not tune the model to score well on the test set**.\n",
        "\n",
        "Different set ratios are used in this approach - a usual example is 60/20/20 train/validation/test.\n",
        "\n",
        "Last week, we skipped the validation test split for simplicity. This week, we're going to include it for performing hyperparameter tuning.\n",
        "\n",
        "We will basically first do the 80/20 train/test, like we did before, and then further divide the 80 train set into 60/20 train and validation set. We'll do it for both scaled and unscaled versions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "erVlA8IQA9MA"
      },
      "outputs": [],
      "source": [
        "#don't do on actor 25\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "############# Unscaled test/train set #############\n",
        "X_train80, X_test, y_train80, y_test = train_test_split(\n",
        "    features,\n",
        "    emotions,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Then further divide the train set into validation\n",
        "# Note that here, to get 60%-20%, we'll need to divide the 80% train set from above to 75-25 split\n",
        "# 0.8 * 0.25 = 0.2\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "    X_train80,\n",
        "    y_train80,\n",
        "    test_size=0.25,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "############ Standard Scaled test/train set ###########\n",
        "# The labels/classes (y_train, y_test) never change, keep old values\n",
        "X_train_scaled80, X_test_scaled, _, _ = train_test_split(\n",
        "    features_scaled,\n",
        "    emotions,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Repeat to get validation, same as above\n",
        "X_train_scaled, X_validation_scaled, _, _ = train_test_split(\n",
        "    X_train_scaled80,\n",
        "    y_train80,\n",
        "    test_size=0.25,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "############# MinMax Scaled test/train set ###############\n",
        "# The labels/classes (y_train, y_test) never change, keep old values\n",
        "X_train_minmax80, X_test_minmax, _, _ = train_test_split(\n",
        "    features_minmax,\n",
        "    emotions,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# And get validation set\n",
        "X_train_minmax, X_validation_minmax, _, _ = train_test_split(\n",
        "    X_train_minmax80,\n",
        "    y_train80,\n",
        "    test_size=0.25,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk83NEWf_-_M"
      },
      "source": [
        "### MLP Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8wqduoWL9hj"
      },
      "source": [
        "We initialize an MLP classification model with random weights and biases at zero, the standard modality. There are techniques for determining optimal weight initialization; the initial distribution of weights is incrediby important because they impact the direction of the gradient and will determine how fast and whether at all a model's loss function will converge.\n",
        "\n",
        "We'll try the off-the-shelf MLP model that comes with sklearn. Using default settings for neural networks is not a good idea in general - and we're going to be using the validation test to check model performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtpfgH_eL9hj",
        "outputId": "efd6e003-a363-418b-8e68-02758ec9b66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible emotions predicted by model:['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
            "Unscaled MLP Model's accuracy on training set is 57.52%\n",
            "Unscaled MLP Model's accuracy on validation set is 45.83%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Default 'off-the-shelf' MLP from sklearn\n",
        "model = MLPClassifier(\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "# Use the split train test to train, we'll first use the unscaled parameters\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'Possible emotions predicted by model:{model.classes_}')\n",
        "print(f'Unscaled MLP Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
        "print(f'Unscaled MLP Model\\'s accuracy on validation set is {100*model.score(X_validation, y_validation):.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDfFZbEcL9hk"
      },
      "source": [
        "Great - we expected terrible performance with no thought given to the model or feature scale. Otherwise, what would a machine learning engineer do? At any rate, we at least know that our model's possible predictions - classes - are correct, and it's even doing a little better than guessing with a 1/8 (12.5%) chance. More than we can ask for with this approach.\n",
        "\n",
        "Let's see how feature scaling impacts the performance of the MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD6n75oBL9hk",
        "outputId": "ea694119-4ccc-4ad8-f04c-f5dd6a9d3249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MinMax scaled MLP Model's accuracy on training set is 63.77%\n",
            "MinMax sacled MLP Model's accuracy on validation set is 53.12%\n",
            "\n",
            "Standard scaled MLP Model's accuracy on training set is 99.19%\n",
            "Standard scaled MLP Model's accuracy on validation set is 66.67%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Fit the model again on the minmax scaled features\n",
        "model.fit(X_train_minmax, y_train)\n",
        "\n",
        "print(f'MinMax scaled MLP Model\\'s accuracy on training set is {100*model.score(X_train_minmax, y_train):.2f}%')\n",
        "print(f'MinMax sacled MLP Model\\'s accuracy on validation set is {100*model.score(X_validation_minmax, y_validation):.2f}%\\n')\n",
        "\n",
        "\n",
        "# Fit the model again on the scaled features\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f'Standard scaled MLP Model\\'s accuracy on training set is {100*model.score(X_train_scaled, y_train):.2f}%')\n",
        "print(f'Standard scaled MLP Model\\'s accuracy on validation set is {100*model.score(X_validation_scaled, y_validation):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmgWuxwoL9hl"
      },
      "source": [
        "That's more like it. Standard scaling is indeed the way to go here. **It is important to consider choice of scaling method, if necessary, as a crucial hyperparameter of a model.** Skipping this step or choosing the wrong scaling method can render even the most appropriate features, worthless.\n",
        "\n",
        "Perhaps with some effort we can make the MLP model work for us. We'll try to give it a fighting chance by actually paying attention to what it's doing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsieghrYL9hl"
      },
      "source": [
        "### Choice of Hyperparameters\n",
        "Before training a network we must select hyperparameters, which determine the network's learning behaviour - hyperparameters determine how the network learns its weights and biases, while the network parameters determine what those weights and biases are. Each type of model (SVC, MLP, Random Forests, etc...) come with their own set of hyperparameters, and each type of optimization algorithm does as well. We may use the same optimization algorithm for two different models, but memorizing the best hyperparameters for one model won't help us with any other model and likely, not any other task even with the same model.\n",
        "\n",
        "The MLP model has a number of hyperparameters significant to its learning behaviour:\n",
        "- **Alpha**: Constrains the model's weights to be within a certain bound to address overfitting; in range [0,1], parameterizes l2 penalty which defines how steeply the cost function modifies weights in proportion to their magnitude, i.e. **regularizes** the cost function and so is also called the regularization term. In practice, **higher alpha more steeply penalizes large weights.**\n",
        "- **Activation function:** Determines the output of a neuron by the transformation applied to the set of inputs to that neuron\n",
        "- **Solver:** Algorithm(s) used for optimization of our weights (with backpropogation **gradient descent** in the case of the MLP classifier)\n",
        "- **Learning rate:** How large of a change the optimization algorithm makes to the model's weights at each training iteration, in\n",
        "- **Epsilon:** Unique to the 'adam' solver, numerical stability - to avoid divison by zero.\n",
        "\n",
        "### Network Architecture\n",
        "The input layer of our MLP neural network is the size of our feature space - that is, one neuron per feature. In our case, 180 input neurons constitute the input layer. Since we are performing multiclass classification, our MLP network has one neuron in its output layer for each class label. In our case 8 output neurons, one per emotion.\n",
        "\n",
        "The size of our hidden layer is of interest and a more dubious task than choosing other hyperparameters, because the behaviour of different numbers and sizes of hidden layers is much less transparent. There are many opinions on the matter, but there is relative consensus in that **many tasks rarely benefit from more than one hidden layer**, i.e. one hidden layer is sufficient in most cases. The number of neurons in that hidden layer is a less obvious choice, but it seems from literature that a number between the input layer and output layer size is a good starting point. At any rate, the time complexity of training our model scales exponentially with the number of hidden layers, so let's keep that number at the minimum, and likely optimum of 1.\n",
        "\n",
        "### Hyperparameter Optimization and Grid Search\n",
        "We're going to take advantage of the grid search cross-validation algorithm to find the best hyperparameters for us.\n",
        "\n",
        "Grid search is relatively simple to understand - we specify a set of candidate values for each hyperparameter, and grid search tries all combinations of those hyperparameter values to build and score potential models against our training and validation data. Grid search scores sets of hyperparameters by building a validation set from the training set we give it.\n",
        "\n",
        "We initialize the MLP classifier with the hyperparameters we will keep constant, those less likely to have a significant impact on the model's performance. We shouldn't use grid search to look for hyperparameters we can easily tune later, such as the number of training iterations (# of forward+backward passes through the network for each _batch_ of training samples); if we do so, we significantly inflate the computational cost with little return. For these hyperparameters we should choose values which are usually reasonable from a literature search (...from stackexchange).\n",
        "\n",
        "In case grid search is too expensive for the hyperparameter grid we specify, **Randomized Grid Search** can be used to sample a fixed number of hyperparameter combinations from the distribution of settings we define.\n",
        "\n",
        "We define a random state so that we can accurately compare any improvements we make to the model.\n",
        "\n",
        "We also define a **batch size**: the number of training samples included in one forward (input) and backward (error) pass of the model, which is one **iteration**. A batch size in range [1, #samples] is common - in our case, [1,1440\\*0.8]. [**Mini-batches**](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) in the set [32, 64, 128, 256] are usually encouraged, especially on smaller datasets. **Smaller batches** cause more frequent weight updates and faster convergence - faster learning - though each update is less accurate compared to a larger batch. **Larger batches** train on a larger set of inputs simultaneously, and so may optimize loss better at each iteration due to a more accurate gradient computation - however, large batches may encourage overfitting and worsen generalization error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9fnJQHzL9hm",
        "outputId": "5c914021-22c1-4901-da6f-e83b1568aeb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd[CV 2/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "\n",
            "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n",
            "[CV 1/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.405 total time=  15.8s\n",
            "[CV 3/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 2/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.560 total time=  15.9s\n",
            "[CV 4/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 3/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.461 total time=  13.9s\n",
            "[CV 5/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 4/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.530 total time=  14.5s\n",
            "[CV 6/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 5/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.496 total time=  14.6s\n",
            "[CV 7/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 6/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.461 total time=  14.0s\n",
            "[CV 8/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 7/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.496 total time=  17.7s\n",
            "[CV 9/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 8/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.417 total time=  18.5s\n",
            "[CV 10/10; 1/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 9/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.539 total time=  14.7s\n",
            "[CV 1/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 1/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.400 total time=  14.7s\n",
            "[CV 2/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 1/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.595 total time=  15.2s\n",
            "[CV 3/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 2/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.672 total time=  15.6s\n",
            "[CV 4/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 3/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.626 total time=  15.5s\n",
            "[CV 5/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 4/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.670 total time=  15.3s\n",
            "[CV 6/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 5/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.661 total time=  14.5s\n",
            "[CV 7/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 6/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.643 total time=  15.0s\n",
            "[CV 8/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 7/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.652 total time=  14.6s\n",
            "[CV 9/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 8/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.600 total time=  15.2s\n",
            "[CV 10/10; 2/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 9/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.652 total time=  13.9s\n",
            "[CV 1/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 10/10; 2/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.617 total time=  14.6s\n",
            "[CV 2/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 1/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.405 total time=  14.0s\n",
            "[CV 3/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 2/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.560 total time=  14.5s\n",
            "[CV 4/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 3/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.461 total time=  14.2s\n",
            "[CV 5/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 4/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.530 total time=  15.1s\n",
            "[CV 6/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 5/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.496 total time=  14.6s\n",
            "[CV 7/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 6/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.461 total time=  15.4s\n",
            "[CV 8/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 7/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.496 total time=  14.4s\n",
            "[CV 9/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 8/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.417 total time=  14.7s\n",
            "[CV 10/10; 3/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 9/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.539 total time=  14.9s\n",
            "[CV 1/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 10/10; 3/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.400 total time=  14.5s\n",
            "[CV 2/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 1/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.595 total time=  15.1s\n",
            "[CV 3/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 2/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.672 total time=  15.8s\n",
            "[CV 4/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 3/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.626 total time=  16.1s\n",
            "[CV 5/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 4/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.670 total time=  14.6s\n",
            "[CV 6/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 5/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.661 total time=  14.1s\n",
            "[CV 7/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 6/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.643 total time=  14.9s\n",
            "[CV 8/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 7/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.652 total time=  14.4s\n",
            "[CV 9/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 8/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.600 total time=  15.8s\n",
            "[CV 10/10; 4/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 9/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.652 total time=  16.3s\n",
            "[CV 1/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 10/10; 4/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.617 total time=  16.8s\n",
            "[CV 2/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 1/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.466 total time=  25.1s\n",
            "[CV 3/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 2/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.517 total time=  28.0s\n",
            "[CV 4/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 3/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.478 total time=  24.5s\n",
            "[CV 5/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 4/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.548 total time=  23.9s\n",
            "[CV 6/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 5/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.557 total time=  22.7s\n",
            "[CV 7/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 6/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.487 total time=  22.9s\n",
            "[CV 8/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 7/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.470 total time=  25.0s\n",
            "[CV 9/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 8/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.426 total time=  21.6s\n",
            "[CV 10/10; 5/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 9/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.557 total time=  23.0s\n",
            "[CV 1/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 5/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.452 total time=  24.7s\n",
            "[CV 2/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 1/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.647 total time=  35.3s\n",
            "[CV 3/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 2/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.707 total time=  42.6s\n",
            "[CV 4/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 3/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.626 total time=  33.8s\n",
            "[CV 5/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 4/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.678 total time=  28.0s\n",
            "[CV 6/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 5/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.748 total time=  25.0s\n",
            "[CV 7/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 6/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.670 total time=  23.0s\n",
            "[CV 8/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 7/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.678 total time=  22.8s\n",
            "[CV 9/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 8/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.617 total time=  25.8s\n",
            "[CV 10/10; 6/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 9/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.670 total time=  25.9s\n",
            "[CV 1/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 10/10; 6/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.600 total time=  24.9s\n",
            "[CV 2/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 1/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.466 total time=  22.0s\n",
            "[CV 3/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 2/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.517 total time=  22.7s\n",
            "[CV 4/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 3/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.478 total time=  30.5s\n",
            "[CV 5/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 4/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.548 total time=  29.3s\n",
            "[CV 6/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 5/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.557 total time=  24.1s\n",
            "[CV 7/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 6/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.487 total time=  25.9s\n",
            "[CV 8/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 7/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.470 total time=  31.9s\n",
            "[CV 9/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 8/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.426 total time=  30.4s\n",
            "[CV 10/10; 7/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 9/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.557 total time=  26.0s\n",
            "[CV 1/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 10/10; 7/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.452 total time=  22.1s\n",
            "[CV 2/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 1/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.647 total time=  25.9s\n",
            "[CV 3/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 2/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.707 total time=  26.1s\n",
            "[CV 4/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 3/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.626 total time=  24.2s\n",
            "[CV 5/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 4/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.678 total time=  21.3s\n",
            "[CV 6/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 5/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.748 total time=  26.3s\n",
            "[CV 7/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 6/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.670 total time=  27.6s\n",
            "[CV 8/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 7/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.678 total time=  25.6s\n",
            "[CV 9/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 8/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.617 total time=  26.0s\n",
            "[CV 10/10; 8/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 9/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.670 total time=  24.9s\n",
            "[CV 1/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 10/10; 8/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.600 total time=  27.9s\n",
            "[CV 2/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 1/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.457 total time=  16.8s\n",
            "[CV 3/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 2/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.569 total time=  16.5s\n",
            "[CV 4/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 3/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.417 total time=  22.6s\n",
            "[CV 5/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 4/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.530 total time=  18.3s\n",
            "[CV 6/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 5/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.504 total time=  16.6s\n",
            "[CV 7/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 6/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.504 total time=  17.2s\n",
            "[CV 8/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 7/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.496 total time=  15.6s\n",
            "[CV 9/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 8/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.452 total time=  14.7s\n",
            "[CV 10/10; 9/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 9/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.513 total time=  16.0s\n",
            "[CV 1/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 9/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.452 total time=  15.9s\n",
            "[CV 2/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 1/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.655 total time=  17.4s\n",
            "[CV 3/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 2/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.690 total time=  20.2s\n",
            "[CV 4/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 3/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.670 total time=  17.9s\n",
            "[CV 5/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 4/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.678 total time=  18.4s\n",
            "[CV 6/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 5/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.670 total time=  18.2s\n",
            "[CV 7/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 6/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.652 total time=  17.7s\n",
            "[CV 8/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 7/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.696 total time=  16.3s\n",
            "[CV 9/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 8/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.600 total time=  15.9s\n",
            "[CV 10/10; 10/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 9/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.626 total time=  16.0s\n",
            "[CV 1/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 10/10; 10/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.652 total time=  16.1s\n",
            "[CV 2/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 1/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.457 total time=  15.9s\n",
            "[CV 3/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 2/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.569 total time=  16.9s\n",
            "[CV 4/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 3/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.417 total time=  17.1s\n",
            "[CV 5/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 4/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.530 total time=  22.0s\n",
            "[CV 6/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 5/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.504 total time=  15.4s\n",
            "[CV 7/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 6/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.504 total time=  20.3s\n",
            "[CV 8/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 7/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.496 total time=  24.2s\n",
            "[CV 9/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 8/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.452 total time=  29.5s\n",
            "[CV 10/10; 11/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 9/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.513 total time=  26.7s\n",
            "[CV 1/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 10/10; 11/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.452 total time=  34.3s\n",
            "[CV 2/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 1/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.655 total time=  37.9s\n",
            "[CV 3/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 2/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.690 total time=  31.2s\n",
            "[CV 4/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 3/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.670 total time=  27.7s\n",
            "[CV 5/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 4/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.678 total time=  26.0s\n",
            "[CV 6/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 5/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.670 total time=  22.0s\n",
            "[CV 7/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 6/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.652 total time=  33.4s\n",
            "[CV 8/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 7/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.696 total time=  32.3s\n",
            "[CV 9/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 8/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.600 total time=  29.9s\n",
            "[CV 10/10; 12/48] START activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 9/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.626 total time=  29.9s\n",
            "[CV 1/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 1/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.405 total time=  21.3s\n",
            "[CV 2/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 10/10; 12/48] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.652 total time=  26.4s\n",
            "[CV 3/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 2/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.552 total time=  15.9s\n",
            "[CV 4/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 3/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.461 total time=  14.9s\n",
            "[CV 5/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 4/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.530 total time=  11.9s\n",
            "[CV 6/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 5/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.496 total time=  14.0s\n",
            "[CV 7/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 6/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.461 total time=  11.6s\n",
            "[CV 8/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 7/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.496 total time=  13.6s\n",
            "[CV 9/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 8/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.417 total time=  14.8s\n",
            "[CV 10/10; 13/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 9/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.539 total time=  15.7s\n",
            "[CV 1/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 13/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.400 total time=  15.0s\n",
            "[CV 2/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 1/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.595 total time=  15.2s\n",
            "[CV 3/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 2/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.664 total time=  18.7s\n",
            "[CV 4/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 3/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.635 total time=  16.4s\n",
            "[CV 5/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 4/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.661 total time=  13.8s\n",
            "[CV 6/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 5/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.661 total time=  15.1s\n",
            "[CV 7/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 6/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.643 total time=  14.0s\n",
            "[CV 8/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 7/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.652 total time=  14.1s\n",
            "[CV 9/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 8/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.617 total time=  14.0s\n",
            "[CV 10/10; 14/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 9/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.661 total time=  15.0s\n",
            "[CV 1/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 10/10; 14/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.626 total time=  16.3s\n",
            "[CV 2/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 1/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.405 total time=  15.2s\n",
            "[CV 3/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 2/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.552 total time=  13.2s\n",
            "[CV 4/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 3/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.461 total time=  14.8s\n",
            "[CV 5/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 4/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.530 total time=  17.4s\n",
            "[CV 6/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 5/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.496 total time=  24.8s\n",
            "[CV 7/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 6/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.461 total time=  22.8s\n",
            "[CV 8/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 7/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.496 total time=  13.1s\n",
            "[CV 9/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 8/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.417 total time=  14.2s\n",
            "[CV 10/10; 15/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 9/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.539 total time=  14.7s\n",
            "[CV 1/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 10/10; 15/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.400 total time=  14.1s\n",
            "[CV 2/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 1/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.595 total time=  16.1s\n",
            "[CV 3/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 2/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.664 total time=  14.9s\n",
            "[CV 4/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 3/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.635 total time=  13.1s\n",
            "[CV 5/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 4/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.661 total time=  13.8s\n",
            "[CV 6/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 5/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.661 total time=  14.9s\n",
            "[CV 7/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 6/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.643 total time=  15.0s\n",
            "[CV 8/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 7/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.652 total time=  14.7s\n",
            "[CV 9/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 8/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.617 total time=  14.5s\n",
            "[CV 10/10; 16/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 9/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.661 total time=  15.0s\n",
            "[CV 1/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 10/10; 16/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.626 total time=  15.2s\n",
            "[CV 2/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 1/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.466 total time=  25.1s\n",
            "[CV 3/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 2/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.517 total time=  21.9s\n",
            "[CV 4/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 3/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.478 total time=  24.8s\n",
            "[CV 5/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 4/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.548 total time=  23.8s\n",
            "[CV 6/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 5/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.557 total time=  19.8s\n",
            "[CV 7/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 6/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.487 total time=  20.4s\n",
            "[CV 8/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 7/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.470 total time=  21.6s\n",
            "[CV 9/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 8/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.426 total time=  23.9s\n",
            "[CV 10/10; 17/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 9/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.557 total time=  23.5s\n",
            "[CV 1/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 17/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.452 total time=  21.1s\n",
            "[CV 2/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 1/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.629 total time=  26.7s\n",
            "[CV 3/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 2/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.716 total time=  25.4s\n",
            "[CV 4/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 3/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.635 total time=  23.9s\n",
            "[CV 5/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 4/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.678 total time=  25.5s\n",
            "[CV 6/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 5/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.748 total time=  25.1s\n",
            "[CV 7/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 6/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.678 total time=  23.5s\n",
            "[CV 8/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 7/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.696 total time=  26.4s\n",
            "[CV 9/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 8/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.609 total time=  24.9s\n",
            "[CV 10/10; 18/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.600 total time=  21.6s\n",
            "[CV 1/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 9/10; 18/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.687 total time=  21.7s\n",
            "[CV 2/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 2/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.517 total time=  23.4s\n",
            "[CV 3/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 1/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.466 total time=  23.8s\n",
            "[CV 4/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 3/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.478 total time=  21.0s\n",
            "[CV 5/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 4/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.548 total time=  21.8s\n",
            "[CV 6/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 5/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.557 total time=  27.8s\n",
            "[CV 7/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 6/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.487 total time=  27.0s\n",
            "[CV 8/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 8/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.426 total time=  27.9s\n",
            "[CV 9/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 7/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.470 total time=  28.5s\n",
            "[CV 10/10; 19/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd\n",
            "[CV 10/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.452 total time=  25.0s\n",
            "[CV 1/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 9/10; 19/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=sgd;, score=0.557 total time=  26.0s\n",
            "[CV 2/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 1/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.629 total time=  22.9s\n",
            "[CV 3/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 2/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.716 total time=  23.6s\n",
            "[CV 4/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 3/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.635 total time=  24.2s\n",
            "[CV 5/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 4/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.678 total time=  24.9s\n",
            "[CV 6/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 5/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.748 total time=  25.5s\n",
            "[CV 7/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 6/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.678 total time=  26.7s\n",
            "[CV 8/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 7/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.696 total time=  22.7s\n",
            "[CV 9/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 8/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.609 total time=  22.7s\n",
            "[CV 10/10; 20/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam\n",
            "[CV 9/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.687 total time=  27.0s\n",
            "[CV 1/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 10/10; 20/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(300,), learning_rate=constant, solver=adam;, score=0.600 total time=  24.7s\n",
            "[CV 2/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 1/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.457 total time=  18.2s\n",
            "[CV 3/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 2/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.569 total time=  17.2s\n",
            "[CV 4/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 4/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.530 total time=  17.5s\n",
            "[CV 5/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 3/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.409 total time=  19.6s\n",
            "[CV 6/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 5/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.504 total time=  22.4s\n",
            "[CV 7/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 6/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.504 total time=  20.5s\n",
            "[CV 8/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 8/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.452 total time=  16.9s\n",
            "[CV 9/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 7/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.496 total time=  18.1s\n",
            "[CV 10/10; 21/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd\n",
            "[CV 9/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.513 total time=  19.1s\n",
            "[CV 1/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 21/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=sgd;, score=0.452 total time=  18.4s\n",
            "[CV 2/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 2/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.690 total time=  19.3s\n",
            "[CV 3/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 1/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.681 total time=  20.1s\n",
            "[CV 4/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 3/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.670 total time=  19.1s\n",
            "[CV 5/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 4/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.678 total time=  19.7s\n",
            "[CV 6/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 5/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.687 total time=  18.9s\n",
            "[CV 7/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 6/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.678 total time=  18.8s\n",
            "[CV 8/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 7/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.687 total time=  24.1s\n",
            "[CV 9/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam\n",
            "[CV 8/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.591 total time=  24.6s\n",
            "\n",
            "[CV 10/10; 22/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam[CV 9/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.609 total time=  17.3s\n",
            "[CV 1/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 10/10; 22/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, solver=adam;, score=0.643 total time=  17.5s\n",
            "[CV 2/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 1/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.457 total time=  21.5s\n",
            "[CV 3/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 2/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.569 total time=  25.7s\n",
            "[CV 4/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 3/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.409 total time=  22.9s\n",
            "[CV 5/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 4/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.530 total time=  21.2s\n",
            "[CV 6/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 5/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.504 total time=  19.6s\n",
            "[CV 7/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 6/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.504 total time=  17.9s\n",
            "[CV 8/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 7/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.496 total time=  19.8s\n",
            "[CV 9/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 8/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.452 total time=  22.2s\n",
            "[CV 10/10; 23/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd\n",
            "[CV 9/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.513 total time=  20.7s\n",
            "[CV 1/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 10/10; 23/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=sgd;, score=0.452 total time=  19.4s\n",
            "[CV 2/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 1/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.681 total time=  16.5s\n",
            "[CV 3/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 2/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.690 total time=  17.1s\n",
            "[CV 4/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 3/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.670 total time=  19.4s\n",
            "[CV 5/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 4/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.678 total time=  26.8s\n",
            "[CV 6/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 5/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.687 total time=  27.3s\n",
            "[CV 7/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 6/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.678 total time=  27.3s\n",
            "[CV 8/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 7/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.687 total time=  25.5s\n",
            "[CV 9/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 8/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.591 total time=  21.3s\n",
            "[CV 10/10; 24/48] START activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam\n",
            "[CV 9/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.609 total time=  18.3s\n",
            "[CV 1/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 10/10; 24/48] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, solver=adam;, score=0.643 total time=  18.0s\n",
            "[CV 2/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 1/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.336 total time=  15.6s\n",
            "[CV 3/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 2/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.353 total time=  16.6s\n",
            "[CV 4/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 3/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.278 total time=  16.8s\n",
            "[CV 5/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 4/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.400 total time=  17.2s\n",
            "[CV 6/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 5/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.357 total time=  14.8s\n",
            "[CV 7/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 6/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.391 total time=  16.5s\n",
            "[CV 8/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 7/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.357 total time=  15.8s\n",
            "[CV 9/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 8/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.296 total time=  16.0s\n",
            "[CV 10/10; 25/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd\n",
            "[CV 9/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.409 total time=  16.6s\n",
            "[CV 1/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 25/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=sgd;, score=0.278 total time=  16.9s\n",
            "[CV 2/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 1/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.500 total time=  15.8s\n",
            "[CV 3/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 2/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.552 total time=  15.7s\n",
            "[CV 4/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 3/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.487 total time=  15.0s\n",
            "[CV 5/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 4/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.513 total time=  15.9s\n",
            "[CV 6/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 5/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.548 total time=  17.4s\n",
            "[CV 7/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 6/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.513 total time=  16.0s\n",
            "[CV 8/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 7/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.513 total time=  15.8s\n",
            "[CV 9/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 8/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.539 total time=  16.1s\n",
            "[CV 10/10; 26/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam\n",
            "[CV 9/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.522 total time=  18.8s\n",
            "[CV 1/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 10/10; 26/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=adaptive, solver=adam;, score=0.470 total time=  16.8s\n",
            "[CV 2/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 1/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.336 total time=  15.3s\n",
            "[CV 3/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 2/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.353 total time=  15.2s\n",
            "[CV 4/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 3/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.278 total time=  15.3s\n",
            "[CV 5/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 4/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.400 total time=  14.6s\n",
            "[CV 6/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 5/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.357 total time=  14.6s\n",
            "[CV 7/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 6/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.391 total time=  17.3s\n",
            "[CV 8/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 7/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.357 total time=  16.2s\n",
            "[CV 9/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 8/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.296 total time=  15.6s\n",
            "[CV 10/10; 27/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd\n",
            "[CV 9/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.409 total time=  15.0s\n",
            "[CV 1/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 10/10; 27/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=sgd;, score=0.278 total time=  18.0s\n",
            "[CV 2/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 1/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.500 total time=  16.4s\n",
            "[CV 3/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 2/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.552 total time=  16.0s\n",
            "[CV 4/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 3/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.487 total time=  16.1s\n",
            "[CV 5/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 4/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.513 total time=  16.0s\n",
            "[CV 6/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 5/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.548 total time=  16.1s\n",
            "[CV 7/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 6/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.513 total time=  17.2s\n",
            "[CV 8/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 7/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.513 total time=  16.0s\n",
            "[CV 9/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 8/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.539 total time=  16.2s\n",
            "[CV 10/10; 28/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam\n",
            "[CV 9/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.522 total time=  15.7s\n",
            "[CV 1/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 10/10; 28/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(180,), learning_rate=constant, solver=adam;, score=0.470 total time=  16.5s\n",
            "[CV 2/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 1/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.345 total time=  26.1s\n",
            "[CV 3/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 2/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.397 total time=  26.4s\n",
            "[CV 4/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 3/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.252 total time=  26.5s\n",
            "[CV 5/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 4/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.348 total time=  27.4s\n",
            "[CV 6/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 5/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.400 total time=  23.7s\n",
            "[CV 7/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 6/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.374 total time=  24.9s\n",
            "[CV 8/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 7/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.348 total time=  27.2s\n",
            "[CV 9/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 8/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.270 total time=  30.3s\n",
            "[CV 10/10; 29/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd\n",
            "[CV 9/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.374 total time=  28.2s\n",
            "[CV 1/10; 30/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 10/10; 29/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=sgd;, score=0.304 total time=  27.4s\n",
            "[CV 2/10; 30/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 1/10; 30/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.517 total time=  29.3s\n",
            "[CV 3/10; 30/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 2/10; 30/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.552 total time=  28.1s\n",
            "[CV 4/10; 30/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n",
            "[CV 3/10; 30/48] END activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam;, score=0.504 total time=  27.0s\n",
            "[CV 5/10; 30/48] START activation=logistic, alpha=0.001, hidden_layer_sizes=(300,), learning_rate=adaptive, solver=adam\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from joblib import parallel_backend\n",
        "\n",
        "\n",
        "# Now that we know standard scaling is best for our features, we'll use those for our training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_scaled,\n",
        "    emotions,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the MLP Classifier and choose parameters we want to keep constant\n",
        "model = MLPClassifier(\n",
        "    # tune batch size later\n",
        "    batch_size=256, #power of 2, try to be as big as it can get\n",
        "    # keep random state constant to accurately compare subsequent models\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Choose the grid of hyperparameters we want to use for Grid Search to build our candidate models\n",
        "parameter_space = {\n",
        "    # A single hidden layer of size between 8 (output classes) and 180 (input features) neurons is most probable\n",
        "    # It's a bad idea at guessing the number of hidden layers to have\n",
        "    # ...but we'll give 2 and 3 hidden layers a shot to reaffirm our suspicions that 1 is best\n",
        "    'hidden_layer_sizes': [(180,), (300,),(100,50,), (200, 70)], #don't make size too large, takes forever\n",
        "    'activation': ['relu', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'], #calculates grid update + changes model depending on training data\n",
        "    'alpha': [0.001, 0.01], #control for overfitting, slows down learning\n",
        "    'epsilon': [1e-08, 0.1], #usually very small\n",
        "    'learning_rate': ['adaptive', 'constant'] #constant = same learning rate since the begin\n",
        "}\n",
        "\n",
        "# Create a grid search object which will store the scores and hyperparameters of all candidate models\n",
        "grid = GridSearchCV(\n",
        "    model,\n",
        "    parameter_space,\n",
        "    cv=10, # CV here shows how many fold of cross-validation we'll do (10 = repeat training 10 times)\n",
        "    n_jobs=-1,\n",
        "    verbose=10)\n",
        "# Fit the models specified by the parameter grid\n",
        "# Note that, here we used the whole 80 training set because cross-validation holds out validation sets automatically\n",
        "# So we won't be using 60-20 split, GridSearchCV will automatically do it\n",
        "with parallel_backend('multiprocessing'):\n",
        "    grid.fit(X_train, y_train) #search that parameter space\n",
        "\n",
        "\n",
        "# get the best hyperparameters from grid search object with its best_params_ attribute\n",
        "print('Best parameters found:\\n', grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSmmKan0eLm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2CV3cakL9hm"
      },
      "source": [
        "#### RELU Activation Function\n",
        "Grid search chose the (Rectified Linear Unit) relu function as the best activation function The function returns 0 if it receives any negative input, but for any positive value  x, it returns that value back. So it can be written as  f(x)=max(0,x). While our **hidden layer use the relu function** for its activation on each neuron, our **output layer will necessarily use the softmax function** - an exponential normalization function - in order to scale output neuron values to a probability between \\[0,1\\] for each class (emotion). Softmax does this normalization by taking the exponent of the value of each output neuron and dividing that by sum of the exponentiated output values, producing _n_ probabilities for _n_ classes. We then choose the highest probability as the predicted class for a set of input features. To be clear, **softmax activation is applied to the values of _all_ output neurons _once_ - not to each neuron.**    \n",
        "\n",
        "#### Adam Optimization Algorithm\n",
        "The solver chosen is the **Adam optimization** algorithm, a variant of **stochastic gradient descent** (SGD). Unlike SGD, which maintains a constant learning rate throughout each training iteration, Adam actually 'adapts', or varies the learning rate by taking into account the moving averages of the first and second moments (mean and variance) of the gradient at each training iteration. It makes sense that grid search chose the more sophisticated algorithm - note however it's not necessarily better than SGD for all tasks.  \n",
        "\n",
        "#### Architecture\n",
        "As expected, the ideal architechture involves just one hidden layer - though with double the neurons we expected for 180 features. Training the 2 and 3 hidden layer models was extremely computationally expensive compared to 1 hidden layer - and provided absolutely no benefit, only complexity and wasted resources. A good reinforcement of the power of simplicity, especially in deep neural networks. In the domain of model simplicity, there is also a compelling argument to be made for interpretability (to the point a model should _never_ be a black box) ...but that's for another time.\n",
        "\n",
        "We can now initialize our MLP model with the best hyperparameters for the task of speech emotion classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czvzUrltL9hn"
      },
      "source": [
        "## Training and Evaluating the MLP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6uw8jnnL9hn"
      },
      "outputs": [],
      "source": [
        "# Now we know best parameters from the Hyperparameter tuning\n",
        "# We can use them for the final model\n",
        "model = MLPClassifier(\n",
        "    activation='relu',\n",
        "    solver='adam', #stops training when getting overfitting\n",
        "    alpha=0.001,\n",
        "    batch_size=256,\n",
        "    hidden_layer_sizes=(300,),\n",
        "    learning_rate='adaptive',\n",
        "    max_iter=1000, # I've found for this task, loss converges at ~1000 iterations (each sigle file getting visited, if visited too much --> overfitting)\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# In the final model, we can use the whole training dataset\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'MLP Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
        "# As we're done with hyperparameter tuning, we can check accuracy on test set\n",
        "# Note that, up to this point, we did not touch the test set at all\n",
        "print(f'MLP Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%')\n",
        "\n",
        "#test on scaled actor 25 to print(f'MLP Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKi-0cFblsP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAthU_-uL9hn"
      },
      "source": [
        "That's a little better - but not great. That's expected, because most of the hyperparameters chosen by grid search are default with sklearn's MLP. At any rate, our MLP model is **overfitting the training data and not generalizing well to the test set**. With the near-perfect training accuracy it's clear our model's loss function is converging - let's see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuLsse6-L9hn"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot([num for num in range(1,model.n_iter_ +1)], model.loss_curve_)\n",
        "plt.title('Loss Curve for MLP Model')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1VQlJ7cL9ho"
      },
      "source": [
        "**Loss is indeed converging, and it appears to be a good learning rate - we want to avoid too high a learning rate since the model will bias data towards the last batches passed through it, and avoid too slow a learning rate because our model will take too long to converge.** Because of its perfect performance on the training data and poor performance on test data, we at this point suspect our model has too high a variance: It's learning to fit its weights so precisely to the training data to score well on it that its performance doesn't extend past that training data. If we had seen poor performance on both training and test sets, we would suspect our model to have a high bias.\n",
        "\n",
        "**To solve high variance, wherein our model is overfitting to noise present in the training data, we can decrease the number of input features and/or increase the size of the training set** so our model has more data to learn on and can generalize better to data it has not seen. **We can also use data regularization, using data augmentation techniques such as the addition of random noise to the audio samples.** Data regularization is different from weight regularization, but they work towards the same purpose of reducing overfitting.\n",
        "\n",
        "**To solve high bias, wherein our model is underfitting to the data, we could increase the number of input features** so that it can learn a better fit to the underlying data.\n",
        "\n",
        "**High variance makes the model's predictions inconsistent**, while **high bias makes them inaccurate** and vice versa, respectively. Ultimately, we aim to make a model that is both accurate and consistent: low variance and low bias with good performance. There is almost always a trade-off between the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zmz85jdL9ho"
      },
      "source": [
        "### The Confusion Matrix\n",
        "A confusion matrix describes the performance of a classification model on test data. The axes of the matrix are labelled with all possible classes, in our case emotions - **the vertical levels/ columns represents predicted classes while the horizontal levels/rows represents the ground truth. The intersection of emotions on the matrix diagonal are correctly predicted labels.** All off-diagonal elements are incorrect predictions. We are literally checking to see where our model is confused, i.e. making incorrect predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_jYs-V_L9ho"
      },
      "outputs": [],
      "source": [
        "#test for actor 25\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn # I like seaborn's confusion matrix over sklearn's default\n",
        "\n",
        "# get predictions on test set\n",
        "test_emotion_predictions = model.predict(X_test) #put actor 25 variable here\n",
        "test_emotion_groundtruth = y_test\n",
        "\n",
        "# set labels for matrix axes from emotions\n",
        "emotion_list =['neutral','calm','happy','sad','angry','fearful','disgust','surprised']\n",
        "emotion_name = [emotion for emotion in emotion_list]\n",
        "\n",
        "# build confusion matrix and normalized confusion matrix\n",
        "conf_matrix = confusion_matrix(test_emotion_groundtruth, test_emotion_predictions, labels=emotion_list)\n",
        "conf_matrix_norm = confusion_matrix(test_emotion_groundtruth, test_emotion_predictions,normalize='true', labels=emotion_list)\n",
        "\n",
        "\n",
        "\n",
        "# make a confusion matrix with labels using a DataFrame\n",
        "confmatrix_df = pd.DataFrame(conf_matrix, index=emotion_name, columns=emotion_name)\n",
        "confmatrix_df_norm = pd.DataFrame(conf_matrix_norm, index=emotion_name, columns=emotion_name)\n",
        "\n",
        "# plot confusion matrices\n",
        "plt.figure(figsize=(16,6))\n",
        "sn.set(font_scale=1.8) # emotion label and title size\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Confusion Matrix')\n",
        "sn.heatmap(confmatrix_df, annot=True, annot_kws={\"size\": 18}) #annot_kws is value font\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "sn.heatmap(confmatrix_df_norm, annot=True, annot_kws={\"size\": 13}) #annot_kws is value font\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7mCBPRSL9hp"
      },
      "source": [
        "The sum of elements in the left matrix is size of our test set, which is the length of y_test. That should be 0.2*1435 = 287. The matrix on the right shows each element as a percentage of samples in that class. Each row adds up to 100%, i.e. each row represents all samples of a particular emotion.\n",
        "\n",
        "**We can tell the model is most accurate at predicting 'calm', and least accurate at predicting 'happy'**. Based on this we might look into our features to see where they are not distinct enough between confusing classes and whether we can rethink our feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7kwQGVtL9hp"
      },
      "source": [
        "### Precision, Recall, F-Score\n",
        "The confusion matrix is an intuitive measure of the precision and recall of our model w.r.t. each class. **Precision is a measure of how many positive predictions are true positives, and recall is a measure of how many positives we actually predicted from all positive samples in the dataset: Lower precision means we have more false positives, while lower recall means we have more false negatives.** For this dataset, precision of each emotion is how often we correctly predict it  and recall is how many of that emotion we predicted out of all samples with that emotion in the dataset.\n",
        "\n",
        "**Some tasks are more concerned with maximizing precision - minimizing false positives - such as predicting whether a prisoner is likely to reoffend if paroled.** It is [catastrophic](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/) to deny an inmate parole due to a false positive.\n",
        "\n",
        "**Some tasks are more concerned with maximizing recall - minimizing false negatives - such as predicting cancer risk from biomarkers.** It is better to inconvenience 10 people with a false positive test so we can catch the 1 true positive who would otherwise have been a false negative, rather than skip the 11 tests altogether (with 10 true negatives and 1 false negative).\n",
        "\n",
        "**The [F-score](https://en.wikipedia.org/wiki/F1_score) is a weighted average of precision and recall when we are equally concerned with both metrics.**\n",
        "\n",
        "**We can also make an [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#:~:text=A%20receiver%20operating%20characteristic%20curve,why%20it%20is%20so%20named.) by plotting True Positive vs False Positive Rate to visualize their trade-off and make considerations for further tuning.** However, ROC curves are usually plotted for binary classification; doing so for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjMV8LA4L9hp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'Test Set Accuracy score =  {100*accuracy_score(y_test, y_pred):.3f}%') #same as model.score(X_test, y_test)\n",
        "print(f'Test Set Precision score =  {100*precision_score(y_test, y_pred, average=\"macro\"):.3f}%') #correct num of cases\n",
        "print(f'Test Set Recall score =  {100*recall_score(y_test, y_pred, average=\"macro\"):.3f}%')\n",
        "print(f'Test Set F-score score =  {100*f1_score(y_test, y_pred, average=\"macro\"):.3}%') #inbetween precision + recall\n",
        "#can have score for each class\n",
        "#importance of each score depends on the case outcome"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onGqbVhcL9hq"
      },
      "source": [
        "**Pretty balanced across the board - about the same false positive rate and false negative rate.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WwFw8bPL9hq"
      },
      "source": [
        "### K-Fold Cross-Validation\n",
        "Finally, to highlight the importance of proper model validation and get a better idea of our model's performance,\n",
        "we're going to use K-Fold Cross-Validation (K-Fold CV). We split our training dataset into _K_ unique validation sets (_K_ sets/folds) - the train/validation split being determined by _K_ where each _validation set = (100/K)%_ of the entire dataset, the training set being composed of the remaining _K-1_ validation sets. The term cross-validation refers to validating the model on multiple validation sets.\n",
        "\n",
        "The terminology here may be somewhat confusing, because we usually **split our 80% _training_ set into a _training_ and _validation_ set at each iteration of K-Fold CV.** We reserve the actual _test_ set - the one we made originally with an 80/20 split of the entire dataset - for checking our model's performance _after_ we have tuned its hyperparameters.\n",
        "\n",
        "In this way, K-Fold CV will train and score _K_ different versions of our classifier.\n",
        "Note that while training sets overlap (unless otherwise defined, e.g. sklearn's GroupKFold), validation sets never overlap.\n",
        "We'll use 10-fold CV, with _K=10_ being a choice giving average scores of models with fairly low bias and moderate variance due in part to the resulting 90/10 train/validation ratio.\n",
        "\n",
        "**K-Fold CV trains our model on _K=10_ different, overlapping training folds and check its performance against _K=10_ validation folds.**\n",
        "\n",
        "Smaller datasets would theoretically be better scored with **higher _K_,** meaning we will have a larger portion of our dataset in training sets and a greater overlap of training samples between folds - this way we better represent a small dataset, have more training data, and hopefully provide a **better estimate of the model's true generalization error**, though at the cost of **higher variance because the _K_ estimates of our model's performance are correlated** (though there are exceptions). In the extreme case, and for computationally inexpensive models, **leave-one-out K-Fold CV (LOOCV)** can be used to construct training folds consisting of all samples-1, and 1 sample with in the validation fold; _each training fold differs by just 1 sample_ and this method provides adequate training data for each model validation on smaller training datasets.\n",
        "\n",
        "Sine we're training a classifier, we use **StratifiedKFold which preserves the percentage of samples in each class** (emotion) for each fold. Although we have a balanced dataset, Stratified K-Fold CV is especially important when classifying an imbalanced dataset. We also set shuffle=True to shuffle the order of sample classes in each fold to match the default behaviour of sklearn's train_test_split so we can accurately compare that to the K-Fold CV score.\n",
        "\n",
        "Choose _K_ for our K-Fold CV and train the MLP classifier on each set of train/validation folds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkJ06_ENL9hq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "kfold = StratifiedKFold(\n",
        "    n_splits=10,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "scores = []\n",
        "X_train=pd.DataFrame(X_train)\n",
        "y_train=pd.DataFrame(y_train)\n",
        "for train_indices, validation_indices in kfold.split(X_train,y_train):\n",
        "\n",
        "\n",
        "\n",
        "    train_x=X_train.iloc[train_indices]\n",
        "    train_y=y_train.iloc[train_indices]\n",
        "\n",
        "    validation_x=X_train.iloc[validation_indices]\n",
        "    validation_y=y_train.iloc[validation_indices]\n",
        "    # fit model to training fold\n",
        "    model.fit(train_x, train_y)\n",
        "    # score the model on validation fold\n",
        "    scores.append(model.score(validation_x, validation_y))\n",
        "\n",
        "print('KFold CV scores for MLP:');[print(f'{(100*score):.2f}%') for score in scores]\n",
        "print(f'Mean score of KFold CV for MLP: {100*np.mean(scores):.2f}%  {100*np.std(scores):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5OSaLjyL9hq"
      },
      "source": [
        "Indeed, quite a bit worse, but a more accurate evaluation of this model. **Because we get considerably worse performance fitting our model on random subsets of our training data, we could surmise that our model's performance is inflated by overfitting** when we train and test it just once on a regular 80/20 split.\n",
        "\n",
        "Although K-Fold CV is computationally expensive, we're getting a lot more insight out of our data and that's a serious advantage when we have very few training samples. Tuning a model to just one validation set, such as in a 60/20/20 split may provide artifically inflated performance metrics which will disappoint when the model is applied to real-world data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjjwVvEZL9hr"
      },
      "source": [
        "We can use a learning curve to compare the performance of our model if we had trained it on smaller subsets of our training dataset, and thus get an idea of whether we might expect a performance increase by using more training data, or much less likely that we don't need as much data.\n",
        "\n",
        "We specify variable sizes of training sets to use for the learning curve to make one model for each size. Remember that since we're using 0.8\\*1440 = 1152 samples in our training set, that is the upper bound that the learning curve can check for us. Like the validation curve, sklearn's learning curve implements Stratified K-Fold CV to evaluate multiclass classification models, so again we specify _K = 10_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RaC-LS5L9hs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Build learning curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    model,\n",
        "    X=X_train,\n",
        "    y=y_train,\n",
        "    cv=10,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    n_jobs=4,\n",
        "    # We have (0.8*1440)/10 = 115/1150 training samples, 1035/1150 test samples\n",
        "    train_sizes=[10,100,300,500,800, 1000] # bounded at 1035 for this dataset for 10-fold cv\n",
        ")\n",
        "# Get errors for learning curve\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure()\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                     color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "plt.title('Learning Curve for MLP Model')\n",
        "plt.xlabel('Number of Training Samples')\n",
        "plt.ylabel('Score')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma7shatAL9hs"
      },
      "source": [
        "One thing the learning curve tells us is that the size of the dataset isn't our biggest issue - our **accuracy is plateauing and wouldn't seriously benefit from a larger training set** - maybe 5% at most with a gigantic dataset. Again, the gap between the training score curve and cross-validation score curve shows us that the model has an extremely high variance and - it scores perfectly on training data but poorly in cross-validation because it is seriously overfit - the model does not generalize well at all on test data.\n",
        "\n",
        "It has become clear that an MLP network may not be the best choice of model for this task - it seems to not have the complexity we would need to properly discriminate the differences between our features w.r.t. emotion. It appears at this point that we would need a considerably more sophisticated deep neural net to get better performance on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyAD_OsSL9hs"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "The MLPClassifier is powerful in that it achieves appreciable performance with relatively little effort invested in exploratory analysis, hyperparameter optimization, and model architechture; especially taking advantage of grid search, and much more so when we know how to tune each hyperparameter individually.\n",
        "\n",
        "We're going to have to explore more complicated deep learning methods to get real performance on this dataset. **Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNNs) and Convolutional Neural Networks (CNNs) are excellent DNN candidates for audio data classification: LSTM RNNs because of their excellent ability to interpret sequential data such as the audio waveform represented as a time series; CNNs because features engineered on audio data such as spectrograms have marked resemblance to images, in which CNNs excel at recognizing and discriminating between distinct patterns.**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "PyCharm (Speech Classifier)",
      "language": "python",
      "name": "pycharm-6a34225"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}